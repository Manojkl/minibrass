\lstinputlisting{../../source-code/minizinc/spd_worse.mzn}

\lstinputlisting{../../source-code/minizinc/tpd_worse.mzn}

Some points that should be emphasized 

\begin{enumerate}
\item Variable ordering can be used to assign reified variables first.
\item Order them decreasingly by importance (can be done using data manipulation in MiniZinc)
\item Assign \texttt{true} first, hoping to find a solution to all soft constraints.
\item Discuss/prove that this search tree, in a static ordering, assures that violation degrees
are tried in TPD-order.
\item SPD-order can probably achieved by using a restarts strategy $\rightarrow$ use new scope 
every time, add only those soft constraints that are supposed to hold.
\item Discuss similarities/differences to conflict-directed A* search.
\item \emph{Quite a nifty point:} in addition to posting the constraint 
\texttt{xpd\_better(lb, violatedScs)}, meaning that the set of violated soft constraints should
be better in the next solution, we can post something like \texttt{penalty(lb) > penalty(violatedScs)}
\emph{as well}! Why does that make sense?
\begin{itemize}
\item \emph{If} we find a solution $\theta_2$ that is XPD-better than $\theta_1$, then it is also penalty-better (by implication).
\item \texttt{violatedScs} is a bounded set variable (at least below by $\emptyset$, above by \texttt{soft-constraints}).
From these bounds, the solver can immediately derive bounds for \texttt{penalty(violatedScs)}. Propagation of reified variables
during search (one satisfied constraint implying the violation of another etc.) is then properly handled. Search can stop,
once the best case minimal penalty violation cannot go below the imposed upper bound for violation.
\item This constraint is \emph{redundant}, i.e., if for any solution $\theta_2$ XPD-better $\theta_1$ holds, penalty better holds
as well.
\item But we do not have a \emph{propagator} for XPD-better. The set variable \texttt{violatedScs} is bounded below by the number
of minimally (definitely, already) violated soft constraints, above by the maximally possible violations (cmp. $\alpha$, $\zeta$).
We could, in principle, build a simple bounds propagator for XPD-better that restricts the domain of  \texttt{violatedScs}
to values strictly above the last found lower bound.
\item It's unreasonable to assume a default behavior like bounds propagation on a user-defined predicate such as XPD-better.
It could be that the predicate is only true ``somewhere'' in the middle between lower and upper set-bound. Then bounds propagation
would incorrectly cut partial assignments.
\item But in lieu of a dedicated XPD-propagator, we can benefit from the redundant penalty constraint. For instance, suppose 
we have seen a solution violating $\{c_2,c_3\}$, with penalty $2$. Assume we are in a search tree and the lower bound for our \texttt{violatedScs}
is $\{c_1\}$ violated constraints, and the upper bound be $\{c_1,c_2,c_3\}$. Thus, the penalty is at least $3$, at most $5$. We can cut the
search since we posted \texttt{penalty(violatedScs) < 2}.
\item It turns out that propagation is smarter than I thought \ldots since the witness is just a bunch of additional variables we get much more
propagation than initially thought, which is nice.
\item The penalty trick still cuts down the number of failures.
\end{itemize}
\item Global constraints aren't as easily reified. Probably in the future \todo{look for reification of global constraints paper, 
there is also something about that in one of the MZ functions papers!}. Better use a restarts approach for now.
\item It turns out that, since XDP-better is formulated as a MiniZinc predicate in terms of a conjunction of other (global) constraints, we inherit some propagation capability.
\item MiniZinc is expressive enough to encapsulate utility methods as MiniZinc functions. That includes a consistency check
to guarantee that the entered graph is not cyclic, that edges and vertex sets are consistent (no edge pointing to an integer
other than one supported by the set of vertices), as well as a function that calculates the transitive closure
of a given graph. This is necessary to obtain a partial order (see \emph{free} partial order over a dag). 
\end{enumerate}

\subsection{Search}

The first search strategy corresponds to classical branch-and-bound~(BAB) search
in propagation engines. For every found solution, a constraint is imposed that 
the next solution has to be strictly better.

\begin{lstlisting}
function ann: strictlyBetterBAB(var set of SOFTCONSTRAINTS: violatedScs) =
       repeat(
           if next() then
               let {
                 set of SOFTCONSTRAINTS: lb = sol(violatedScs); 
               } in (
                 print("Intermediate solution:") /\ print() /\
                 commit() /\ post(spd_better(lb, violatedScs, 
                                  SOFTCONSTRAINTS, edges))
               )
           else break endif
       );
       
[...]
       
search strictlyBetterBAB_TPD(violatedScs) /\ 
   if hasSol() then print("Final solution: ") /\ print() 
   else print("No solution found\n") endif;
\end{lstlisting}

While this procedure yields optimal solutions, it is not ideal for partially ordered objectives since 
another optimum need not be better than the current solution. Instead, we need to impose that any coming 
solution \emph{must not be dominated} by \emph{any} solution seen so far. Technically,
we have a set of lower bounds (the satisfaction degrees of previous solutions) $L = \{l_1, \ldots, l_m \}$ and
require that it must not be the case that $\exists l \in L : \mathrm{obj} \leq_M l$ for any partial valuation
structure $M$. The next solution must either be strictly better than any one of the maxima of $L$ or incomparable to
all of them. We would also like to remove lower bounds from $L$ when we find stricter ones (as one would do with
totally ordered objective spaces) but this is not as easy due to partiality (which constraint was to blame/should
stop being imposed).

\begin{lstlisting}
function ann: onlyNotDominated(var set of SOFTCONSTRAINTS: violatedScs) =
       repeat(
           if next() then
               let {
                 set of SOFTCONSTRAINTS: lb = sol(violatedScs); 
               } in (
                 print("Intermediate solution:") /\ print() /\
                 commit() /\ post(not (spd_better(violatedScs, lb, 
                                         SOFTCONSTRAINTS, edges) 
                                         \/ violatedScs = lb ) )
               )
           else break endif
       );
\end{lstlisting}

The difference is best explained by an example. Consider the following oversimplified constraint model.
\begin{lstlisting}
var 1..3: x; 

constraint x = 1 <-> violated[1];
constraint x = 2 <-> violated[2];
constraint x = 3 <-> violated[3];

solve 
:: int_search([x], input_order, indomain_max, complete)
search strictlyBetterBAB_TPD(violatedScs);
\end{lstlisting}
We explore $\concVar{x}$ in a decreasing order. This 
results in the sequence $\seq{ \{3\}, \{2\}, \{1\} }$ of 
satisfaction degrees. $\set{3}$ and $\set{2}$ both 
dominate $\set{1}$ but are incomparable; $\set{1} <_M \set{3}$,
$\set{1} <_M \set{2}$ but $\set{2} \parallel_M \set{3}$. The reachable optima
of this problem are clearly $\set{ \set{2}, \set{3} }$
\begin{verbatim}
Intermediate solution:Obj: 1 by violating {3..3 } : x -> 3
----------
==========
\end{verbatim}
Each assignment to $\concVar{x}$ violates precisely one soft constraint. 